{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad43d18e",
   "metadata": {},
   "source": [
    "# 0. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77b67e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a5be9",
   "metadata": {},
   "source": [
    "Project Sentiment Analysis bertujuan untuk mengembangkan sistem cerdas berbasis teknologi AI untuk mengklasifikasi sentiment pada twitter dengan metode text analysis. Sistem ini akan membantu memperoleh **kecenderungan sentimen opini positif, negatif atau netral** dari pengguna di platform tersebut.\n",
    "\n",
    "Dataset yang digunakan adalah kumpulan tweet dari pengguna twitter saat pelaksanaan **Pilpres 2019**. Dataset tersebut  didapat melalui proses web scraping. Data terdiri dari 1815 tweet yanng memuat tiga kategori sentimen: positif, netral, dan negatif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468e627",
   "metadata": {},
   "source": [
    "### Alur Pelaksanaan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f61c0",
   "metadata": {},
   "source": [
    "Eksperimen dilakukan dengan menggunakan berbagai ragam teknik preprocessing dan vectorization. Selain itu juga menggunakan **menguji algoritma Random Forest dan LSTM (ditambah hyperparameter tuning untuk model optimization)**.\n",
    "\n",
    "Setelah itu dilakukan model evaluation dan penarikan kesimpulan untuk memilih algoritma terbaik.\n",
    "\n",
    "Setelah project selesai, hasil pengerjaan dapat dipublikasikan ke Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296f3c8",
   "metadata": {},
   "source": [
    "### Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268ddda",
   "metadata": {},
   "source": [
    "- 1-7 Juli: Data & Algoritma understanding\n",
    "- 8-9 Juli: Group Discussion\n",
    "- 10-14 Juli: Model training & Evaluation\n",
    "- 15 Juli: Presentation Preparation\n",
    "- 16 Juli: Project Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f087b28",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb7ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3474a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/tweet.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/tweet.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/tweet.csv'"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('./dataset/tweet.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83a882",
   "metadata": {},
   "source": [
    "# 2. EDA (before text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00397c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c848d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7295dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10b577",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9651c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348585c",
   "metadata": {},
   "source": [
    "## 2.1. Distribution of Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ec29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.sentimen.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27897671",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = {'negatif': 'r', 'netral': '#CCCCCC', 'positif': 'g'}\n",
    "\n",
    "ax = sns.countplot(data=tweets, x='sentimen', palette=color_palette)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(),'.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Distribution of Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af68ef",
   "metadata": {},
   "source": [
    "## 2.2. Number word string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef98cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['num_words'] = tweets['tweet'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "tweets_negatif = tweets[tweets['sentimen']=='negatif']\n",
    "tweets_netral = tweets[tweets['sentimen']=='netral']\n",
    "tweets_positif = tweets[tweets['sentimen']=='positif']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a2c00",
   "metadata": {},
   "source": [
    "### 2.2.1. All Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14c548",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(tweets['num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7291b8",
   "metadata": {},
   "source": [
    "### 2.2.2. negatif sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tweets_negatif['num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c404a0e",
   "metadata": {},
   "source": [
    "### 2.2.3. netral sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af346b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tweets_netral['num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb259ef7",
   "metadata": {},
   "source": [
    "### 2.2.4. positif sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tweets_positif['num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc25a637",
   "metadata": {},
   "source": [
    "# 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb1de7",
   "metadata": {},
   "source": [
    "0. Remove url and punctuation\n",
    "1. Slang words to standard words conversion\n",
    "2. Tokenization\n",
    "3. Stemming & Lemmatization\n",
    "4. Stop words removal\n",
    "5. Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6940c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503d403",
   "metadata": {},
   "source": [
    "### create list stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendStopWordRemoverFactory(StopWordRemoverFactory):\n",
    "    def get_stop_words(self):\n",
    "        \n",
    "        add_sw = ['ga', 'gak', 'yg', 'pak','bapak','ibu','bu','akan', 'utk', 'di', 'untuk', 'itu','selain itu'\n",
    "                 ,\"ud\",\"udah\",\"sdh\",\"sudah\",\"kalau\",\"klo\",\"sy\",\"aing\",\"saia\",\"gue\",\"awak\",\"aq\",\"gua\",\"gw\",\"ak\",\"aku\",\"saya\",\"banget\",\"blibli\",\"tokopedia\",\"shopee\",\"bukalapak\",\"lazada\",\"jdid\",\"zalora\",\"elevenia\",\"sudah\",\"bhinneka\",\"gk\",\"yg\",\"ga\",\"gak\",\"nggak\",\"saja\",\"aja\",\"aj\",\"ada\",\"adalah\",\"adanya\",\"adapun\",\"agak\",\"agaknya\",\"agar\",\"akan\",\"akankah\",\"akhir\",\"akhiri\",\"akhirnya\",\"aku\",\"akulah\",\"amat\",\"amatlah\",\"anda\",\"andalah\",\"antar\",\"antara\",\"antaranya\",\"apa\",\"apaan\",\"apabila\",\"apakah\",\"apalagi\",\"apatah\",\"artinya\",\"asal\",\"asalkan\",\"atas\",\"atau\",\"ataukah\",\"ataupun\",\"awal\",\"awalnya\",\"bagai\",\"bagaikan\",\"bagaimana\",\"bagaimanakah\",\"bagaimanapun\",\"bagi\",\"bagian\",\"bahkan\",\"bahwa\",\"bahwasanya\",\"baik\",\"bakal\",\"bakalan\",\"balik\",\"banyak\",\"bapak\",\"baru\",\"bawah\",\"beberapa\",\"begini\",\"beginian\",\"beginikah\",\"beginilah\",\"begitu\",\"begitukah\",\"begitulah\",\"begitupun\",\"bekerja\",\"belakang\",\"belakangan\",\"belum\",\"belumlah\",\"benar\",\"benarkah\",\"benarlah\",\"berada\",\"berakhir\",\"berakhirlah\",\"berakhirnya\",\"berapa\",\"berapakah\",\"berapalah\",\"berapapun\",\"berarti\",\"berawal\",\"berbagai\"\n",
    "                ,\"berdatangan\",\"beri\",\"berikan\",\"berikut\",\"berikutnya\",\"berjumlah\",\"berkali-kali\",\"berkata\",\"berkehendak\",\"berkeinginan\",\"berkenaan\",\"berlainan\",\"berlalu\",\"berlangsung\",\"berlebihan\",\"bermacam\",\"bermacam-macam\",\"bermaksud\",\"bermula\",\"bersama\",\"bersama-sama\",\"bersiap\",\"bersiap-siap\",\"bertanya\",\"bertanya-tanya\",\"berturut\",\"berturut-turut\",\"bertutur\",\"berujar\",\"berupa\",\"besar\",\"betul\",\"betulkah\",\"biasa\",\"biasanya\",\"bila\",\"bilakah\",\"bisa\",\"bisakah\",\"boleh\",\"bolehkah\",\"bolehlah\",\"buat\",\"bukan\",\"bukankah\",\"bukanlah\",\"bukannya\",\"bulan\",\"bung\",\"cara\",\"caranya\",\"cukup\",\"cukupkah\",\"cukuplah\",\"cuma\",\"dahulu\",\"dalam\",\"dan\",\"dapat\",\"dari\",\"daripada\",\"datang\",\"dekat\",\"demi\",\"demikian\",\"demikianlah\",\"dengan\",\"depan\",\"di\",\"dia\",\"diakhiri\",\"diakhirinya\",\"dialah\",\"diantara\",\"diantaranya\",\"diberi\",\"diberikan\",\"diberikannya\",\"dibuat\",\"dibuatnya\",\"didapat\",\"didatangkan\",\"digunakan\",\"diibaratkan\",\"diibaratkannya\",\"diingat\",\"diingatkan\",\"diinginkan\",\"dijawab\",\"dijelaskan\",\"dijelaskannya\",\"dikarenakan\",\"dikatakan\",\"dikatakannya\",\"dikerjakan\",\"diketahui\"\n",
    "            ,\"diketahuinya\",\"dikira\",\"dilakukan\",\"dilalui\",\"dilihat\",\"dimaksud\",\"dimaksudkan\",\"dimaksudkannya\",\"dimaksudnya\",\"diminta\",\"dimintai\",\"dimisalkan\",\"dimulai\",\"dimulailah\",\"dimulainya\",\"dimungkinkan\",\"dini\",\"dipastikan\",\"diperbuat\",\"diperbuatnya\",\"dipergunakan\",\"diperkirakan\",\"diperlihatkan\",\"diperlukan\",\"diperlukannya\",\"dipersoalkan\",\"dipertanyakan\",\"dipunyai\",\"diri\",\"dirinya\",\"disampaikan\",\"disebut\",\"disebutkan\",\"disebutkannya\",\"disini\",\"disinilah\",\"ditambahkan\",\"ditandaskan\",\"ditanya\",\"ditanyai\",\"ditanyakan\",\"ditegaskan\",\"ditujukan\",\"ditunjuk\",\"ditunjuki\",\"ditunjukkan\",\"ditunjukkannya\",\"ditunjuknya\",\"dituturkan\",\"dituturkannya\",\"diucapkan\",\"diucapkannya\",\"diungkapkan\",\"dong\",\"dua\",\"dulu\",\"empat\",\"enggak\",\"enggaknya\",\"entah\",\"entahlah\",\"guna\",\"gunakan\",\"hal\",\"hampir\",\"hanya\",\"hanyalah\",\"hari\",\"harus\",\"haruslah\",\"harusnya\",\"hendak\",\"hendaklah\",\"hendaknya\",\"hingga\",\"ia\",\"ialah\",\"ibarat\",\"ibaratkan\",\"ibaratnya\",\"ibu\",\"ikut\",\"ingat\",\"ingat-ingat\",\"ingin\",\"inginkah\",\"inginkan\",\"ini\",\"inikah\",\"inilah\",\"itu\",\"itukah\",\"itulah\",\"jadi\",\"jadilah\"\n",
    "            ,\"jadinya\",\"jangan\",\"jangankan\",\"janganlah\",\"jauh\",\"jawab\",\"jawaban\",\"jawabnya\",\"jelas\",\"jelaskan\",\"jelaslah\",\"jelasnya\",\"jika\",\"jikalau\",\"juga\",\"jumlah\",\"jumlahnya\",\"justru\",\"kala\",\"kalau\",\"kalaulah\",\"kalaupun\",\"kalian\",\"kami\",\"kamilah\",\"kamu\",\"kamulah\",\"kan\",\"kapan\",\"kapankah\",\"kapanpun\",\"karena\",\"karenanya\",\"kasus\",\"kata\",\"katakan\",\"katakanlah\",\"katanya\",\"ke\",\"keadaan\",\"kebetulan\",\"kecil\",\"kedua\",\"keduanya\",\"keinginan\",\"kelamaan\",\"kelihatan\",\"kelihatannya\",\"kelima\",\"keluar\",\"kembali\",\"kemudian\",\"kemungkinan\",\"kemungkinannya\",\"kenapa\",\"kepada\",\"kepadanya\",\"kesampaian\",\"keseluruhan\",\"keseluruhannya\",\"keterlaluan\",\"ketika\",\"khususnya\",\"kini\",\"kinilah\",\"kira\",\"kira-kira\",\"kiranya\",\"kita\",\"kitalah\",\"kok\",\"kurang\",\"lagi\",\"lagian\",\"lah\",\"lain\",\"lainnya\",\"lalu\",\"lama\",\"lamanya\",\"lanjut\",\"lanjutnya\",\"lebih\",\"lewat\",\"lima\",\"luar\",\"macam\",\"maka\",\"makanya\",\"makin\",\"malah\",\"malahan\",\"mampu\",\"mampukah\",\"mana\",\"manakala\",\"manalagi\",\"masa\",\"masalah\",\"masalahnya\",\"masih\",\"masihkah\",\"masing\",\"masing-masing\",\"mau\",\"maupun\",\"melainkan\",\"melakukan\",\"melalui\"\n",
    "            ,\"melihat\",\"melihatnya\",\"memang\",\"memastikan\",\"memberi\",\"memberikan\",\"membuat\",\"memerlukan\",\"memihak\",\"meminta\",\"memintakan\",\"memisalkan\",\"memperbuat\",\"mempergunakan\",\"memperkirakan\",\"memperlihatkan\",\"mempersiapkan\",\"mempersoalkan\",\"mempertanyakan\",\"mempunyai\",\"memulai\",\"memungkinkan\",\"menaiki\",\"menambahkan\",\"menandaskan\",\"menanti\",\"menanti-nanti\",\"menantikan\",\"menanya\",\"menanyai\",\"menanyakan\",\"mendapat\",\"mendapatkan\",\"mendatang\",\"mendatangi\",\"mendatangkan\",\"menegaskan\",\"mengakhiri\",\"mengapa\",\"mengatakan\",\"mengatakannya\",\"mengenai\",\"mengerjakan\",\"mengetahui\",\"menggunakan\",\"menghendaki\",\"mengibaratkan\",\"mengibaratkannya\",\"mengingat\",\"mengingatkan\",\"menginginkan\",\"mengira\",\"mengucapkan\",\"mengucapkannya\",\"mengungkapkan\",\"menjadi\",\"menjawab\",\"menjelaskan\",\"menuju\",\"menunjuk\",\"menunjuki\",\"menunjukkan\",\"menunjuknya\",\"menurut\",\"menuturkan\",\"menyampaikan\",\"menyangkut\",\"menyatakan\",\"menyebutkan\",\"menyeluruh\",\"menyiapkan\",\"merasa\",\"mereka\",\"merekalah\",\"merupakan\",\"meski\",\"meskipun\",\"meyakini\",\"meyakinkan\",\"minta\",\"mirip\",\"misal\",\"misalkan\",\"misalnya\",\"mula\",\"mulai\",\"mulailah\",\"mulanya\",\"mungkin\",\"mungkinkah\",\"nah\",\"naik\",\"namun\",\"nanti\",\"nantinya\",\"nyaris\",\"nyatanya\",\"oleh\",\"olehnya\",\"pada\",\"padahal\",\"padanya\",\"pak\",\"paling\",\"panjang\",\"pantas\",\"para\",\"pasti\",\"pastilah\",\"penting\",\"pentingnya\",\"per\",\"percuma\",\"perlu\",\"perlukah\",\"perlunya\",\"pernah\",\"persoalan\",\"pertama\",\"pertama-tama\",\"pertanyaan\",\"pertanyakan\",\"pihak\",\"pihaknya\",\"pukul\",\"pula\",\"pun\",\"punya\",\"rasa\",\"rasanya\",\"rata\",\"rupanya\",\"saat\",\"saatnya\",\"saja\",\"sajalah\",\"saling\",\"sama\",\"sama-sama\",\"sambil\",\"sampai\",\"sampai-sampai\",\"sampaikan\",\"sana\",\"sangat\",\"sangatlah\",\"satu\",\"saya\",\"sayalah\",\"se\",\"sebab\",\"sebabnya\",\"sebagai\",\"sebagaimana\",\"sebagainya\",\"sebagian\",\"sebaik\",\"sebaik-baiknya\",\"sebaiknya\",\"sebaliknya\",\"sebanyak\",\"sebegini\",\"sebegitu\",\"sebelum\",\"sebelumnya\",\"sebenarnya\",\"seberapa\",\"sebesar\",\"sebetulnya\",\"sebisanya\",\"sebuah\",\"sebut\",\"sebutlah\",\"sebutnya\",\"secara\",\"secukupnya\",\"sedang\",\"sedangkan\",\"sedemikian\",\"sedikit\",\"sedikitnya\",\"seenaknya\",\"segala\",\"segalanya\",\"segera\",\"seharusnya\",\"sehingga\",\"seingat\",\"sejak\",\"sejauh\",\"sejenak\",\"sejumlah\",\"sekadar\",\"sekadarnya\",\"sekali\",\"sekali-kali\",\"sekalian\",\"sekaligus\",\"sekalipun\",\"sekarang\",\"sekarang\",\"sekecil\",\"seketika\",\"sekiranya\",\"sekitar\",\"sekitarnya\",\"sekurang-kurangnya\",\"sekurangnya\",\"sela\",\"selain\",\"selaku\",\"selalu\",\"selama\",\"selama-lamanya\",\"selamanya\",\"selanjutnya\",\"seluruh\",\"seluruhnya\",\"semacam\",\"semakin\",\"semampu\",\"semampunya\",\"semasa\",\"semasih\",\"semata\",\"semata-mata\",\"semaunya\",\"sementara\",\"semisal\",\"semisalnya\",\"sempat\",\"semua\",\"semuanya\",\"semula\",\"sendiri\",\"sendirian\",\"sendirinya\",\"seolah\",\"seolah-olah\",\"seorang\",\"sepanjang\",\"sepantasnya\",\"sepantasnyalah\",\"seperlunya\",\"seperti\",\"sepertinya\",\"sepihak\",\"sering\",\"seringnya\",\"serta\",\"serupa\",\"sesaat\",\"sesama\",\"sesampai\",\"sesegera\",\"sesekali\",\"seseorang\",\"sesuatu\",\"sesuatunya\",\"sesudah\",\"sesudahnya\",\"setelah\",\"setempat\",\"setengah\",\"seterusnya\",\"setiap\",\"setiba\",\"setibanya\",\"setidak-tidaknya\",\"setidaknya\",\"setinggi\",\"seusai\",\"sewaktu\",\"siap\",\"siapa\",\"siapakah\",\"siapapun\",\"sini\",\"sinilah\",\"soal\",\"soalnya\",\"suatu\",\"sudah\",\"sudahkah\",\"sudahlah\",\"supaya\",\"tadi\",\"tadinya\",\"tahu\",\"tahun\",\"tak\",\"tambah\",\"tambahnya\",\"tampak\",\"tampaknya\",\"tandas\",\"tandasnya\",\"tanpa\",\"tanya\",\"tanyakan\",\"tanyanya\",\"tapi\",\"tegas\",\"tegasnya\",\"telah\",\"tempat\",\"tengah\",\"tentang\",\"tentu\",\"tentulah\",\"tentunya\",\"tepat\",\"terakhir\",\"terasa\",\"terbanyak\",\"terdahulu\",\"terdapat\",\"terdiri\",\"terhadap\",\"terhadapnya\",\"teringat\",\"teringat-ingat\",\"terjadi\",\"terjadilah\",\"terjadinya\",\"terkira\",\"terlalu\",\"terlebih\",\"terlihat\",\"termasuk\",\"ternyata\",\"tersampaikan\",\"tersebut\",\"tersebutlah\",\"tertentu\",\"tertuju\",\"terus\",\"terutama\",\"tetap\",\"tetapi\",\"tiap\",\"tiba\",\"tiba-tiba\",\"tidak\",\"tidakkah\",\"tidaklah\",\"tiga\",\"tinggi\",\"toh\",\"tunjuk\",\"turut\",\"tutur\",\"tuturnya\",\"ucap\",\"ucapnya\",\"ujar\",\"ujarnya\",\"umum\",\"umumnya\",\"ungkap\",\"ungkapnya\",\"untuk\",\"usah\",\"usai\",\"waduh\",\"wah\",\"wahai\",\"waktu\",\"waktunya\",\"walau\",\"walaupun\",\"wong\",\"yaitu\",\"yakin\",\"yakni\",\"yang\",\"anjing\",\"anjiang\",\"anjir\",\"anjay\",\"anying\",\"asu\",\"asoe\",\"babi\",\"bajingan\",\"banci\",\"bangsat\",\"bego\",\"bengak\",\"berak\",\"bokong\",\"bodoh\",\"bongak\",\"edan\",\"fak\",\"fuck\",\"fakboi\",\"bitch\",\"fap\",\"gigolo\",\"goblok\",\"gila\",\"gilo\",\"jablay\",\"jalang\",\"jancuk\",\"jancok\",\"kampret\",\"kafir\",\"kontol\",\"kentot\",\"dancok\",\"kunyuk\",\"kufar\",\"kimak\",\"homo\",\"maho\",\"memek\",\"monyet\",\"ngentot\",\"pantat\",\"pantak\",\"pantek\",\"pecun\",\"pelakor\",\"ngehe\",\"pelacur\",\"perek\",\"peler\",\"pepek\",\"pukimak\",\"setan\",\"syaithon\",\"saiton\",\"sial\",\"sialan\",\"sinting\",\"sintiang\",\"silit\",\"sontoloyo\",\"tai\",\"telek\",\"tolol\",\"lonte\",\"entut\",\"gimana\",\"widihh\",\"nya\",\"gabisa\",\"nambah\",\"kontoll\",\"ih\",\"sih\",\"selesai\",\"banget\",\"si\",\"knapa\",\"bngt\",\"kamu\",\"an\",\"tanggal\",\"dizalora\",\"in\",\"an\",\"ini\",\"emang\",\"sngt\",\"many\",\"to\",\"thank\",\"you\",\"ko\",\"tumben\",\"lho\",\"segitu\",\"kalau\",\"doang\",\"ayo\",\"doank\",\"rbuan\",\"woi\",\"parahhhh\",\"trus\",\"woy\",\"emg\",\"gausah\",\"udh\",\"gadak\",\"huhh\",\"akh\",\"nanti\",\"belum\",\"cuiiihhh\",\"bangt\",\"selamat\",\"begitu\",\"bhineka\",\"situ\",\"loh\",\"deh\",\"mah\",\"yaa\",\"tapi\",\"the\",\"karena\",\"padabal\",\"ttep\",\"sopi\",\"ealah\",\"mas\",\"kampreeeeet\",\"busetttt\",\"tahi\",\"ah\",\"sich\",\"tokped\",\"lzd\",\"gada\",\"gatau\",\"udh\",\"ttep\",\"ehhh\",\"bosss\",\"wkekwk\",\"bajing\",\"laknat\",\"asuuu\",\"ku\",\"woiii\",\"gueeee\",\"njerr\",\"waduhhhh\",\"ma\",\"dunk\",\"dunk\",\"untuk\",\"dll\",\"brupa\",\"sangatx\",\"niih\",\"bos\",\"eh\",\"smua\",\"gni\",\"bye\",\"ajah\",\"gilee\",\"luu\",\"ndro\",\"dg\",\"kga\",\"min\",\"sm\",\"haisshhh\",\"piye\",\"iki\",\"skali\",\"hehehe\",\"ms\",\"juga\",\"fff\",\"mamam\",\"inibsangat\",\"andri\",\"yopiyanto\",\"broo\",\"donk\",\"tes\",\"si\",\"ter\",\"dech\",\"wow\",\"oh\",\"toped\",\"dahhhh\",\"grab\",\"ijo\",\"oren\",\"ovo\",\"gopay\",\"kredivo\",\"sok\",\"david\",\"gaes\",\"dr\",\"ovonya\",\"bbrp\",\"gmna\",\"bbrpa\",\"and\",\"uah\",\"uhahhhhhh\",\"topeeed\",\"ngak\",\"kamu\",\"anj\",\"nge\",\"lah\",\"linkaja\",\"gosend\",\"sicepat\",\"jne\",\"yah\",\"goto\",\"gofood\",\"goride\",\"prakerja\",\"haduhhhh\",\"pediya\",\"kakak\",\"hhe\",\"bro\",\"bismillah\",\"fuulll\",\"goof\",\"assalamualaikum\",\"atsu\",\"pea\",\"you\",\"agan\",\"semenjak\",\"gojek\",\"lagiii\",\"lagi\",\"bts\",\"kkzara\",\"cog\",\"aaaaa\",\"sich\",\"cih\",\"ciyy\",\"padahal\",\"ad\",\"adlh\",\"ahaha\",\"aj\",\"ak\",\"akika\",\"akkoh\",\"akuwh\",\"alow\",\"anjrit\",\"ap2\",\"apasih\",\"aps\",\"aq\",\"aqueh\",\"q\",\"asem\",\"ato\",\"awak\",\"bakalan\",\"bangedh\",\"bcanda\",\"beud\",\"bg\",\"bgmn\",\"bgt\",\"bkl\",\"bknnya\",\"blum\",\"boljug\",\"boyeh\",\"bs\",\"bt\",\"btw\",\"bwt\",\"byk\",\"can\",\"thanks\",\"d\",\"dah\",\"dapet\",\"de\",\"dek\",\"deyh\",\"dgn\",\"disono\",\"dkk\",\"dlu\",\"dngn\",\"dongs\",\"dpt\",\"dri\",\"drmn\",\"drtd\",\"dst\",\"duh\",\"egp\",\"eke\",\"ane\",\"ente\",\"elu\",\"emangnya\",\"emng\",\"endak\",\"enggak\",\"gaada\",\"gag\",\"gaje\",\"gpp\",\"gan\",\"gbs\",\"geje\",\"ghiy\",\"gimana\",\"githu\",\"gj\",\"gn\",\"gt\",\"gpny\",\"gr\",\"gtau\",\"guoblok\",\"ha\",\"haha\",\"hallow\",\"hehe\",\"helo\",\"he\",\"halo\",\"hey\",\"hai\",\"hny\",\"hrus\",\"imho\",\"iye\",\"ja\",\"jadiin\",\"jdi\",\"jga\",\"jir\",\"k\",\"kagak\",\"kalo\",\"kamuwh\",\"karna\",\"katrok\",\"kayanya\",\"kdu\",\"kepengen\",\"kepingin\",\"kl\",\"klianz\",\"klw\",\"km\",\"knp\",\"kpn\",\"kt\",\"kyk\",\"leh\",\"lgi\",\"lgsg\",\"liat\",\"low\",\"lum\",\"maneh\",\"mao\",\"mw\",\"n\",\"napa\",\"nda\",\"ne\",\"ngapah\",\"ngga\",\"ngmng\",\"nie\",\"nih\",\"niyh\",\"np\",\"ogah\",\"pd\",\"pi\",\"pisan\",\"qmu\",\"ruz\",\"saia\",\"samsek\",\"siech\",\"sj\",\"spt\",\"sgt\",\"shg\",\"scr\",\"sbh\",\"sbnrny\",\"sdgkn\",\"td\",\"thankz\",\"tks\",\"ttg\",\"tuch\",\"tuh\",\"u\",\"urang\",\"yawdah\",\"yasudah\",\"adlah\",\"ajj\",\"akko\",\"akyu\",\"ama\",\"aqu\",\"atuh\",\"ayok\",\"bet\",\"beut\",\"bgd\",\"brur\",\"bapak\",\"ibu\",\"bokap\",\"nyokap\",\"cama\",\"ceu\",\"teh\",\"coz\",\"cpa\",\"guys\",\"kaka\",\"kamuh\",\"kamyu\",\"kau\",\"khan\",\"kk\",\"klian\",\"koq\",\"ky\",\"krn\",\"kykny\",\"lu\",\"lw\",\"maaciw\",\"makasih\",\"mba\",\"mas\",\"nyok\",\"qmo\",\"qt\",\"spy\",\"sy\",\"thanks\",\"thx\",\"tp\",\"tq\",\"trims\",\"mz\",\"alfamart\",\"cm\",\"buruk\",\"lambat\",\"jelek\",\"bagus\",\"ok\",\"oke\",\"kecewa\",\"mantap\",\"manfaat\",\"gblk\",\"ni\",\"keren\",\"bagus\",\"ajg\",\"iya\",\"tidak\",\"muas\",\"puas\",\"goblog\",\"tdk\",\"dancuk\",\"huff\",\"lumayan\",\"mayan\",\"y\",\"eek\",\"belum\",\"blm\",\"nyesel\",\"bahagia\",\"senang\",\"seneng\",\"mantul\",\"good\",\"top\",\"suka\",\"not\",\"bad\",\"kak\",\"okey\",\"gitu\",\"coba\",\"sip\",\"nice\",\"job\",\"thanks\",\"terima kasih\",\"moga\",\"lasada\",\"jd\",\"id\",\"parah\",\"for\",\"sumpah\",\"best\",\"bli\",\"payah\",\"blibi\",\"blipay\",\"gg\",\"love\",\"amazing\",\"mantab\",\"terimakasih\",\"cakep\",\"very\",\"bgs\",\"jos\",\"is\",\"euy\",\"gandos\",\"of\",\"waw\",\"biar\",\"buka\",\"lapak\",\"terima\",\"kasih\",\"alhamdulillah\",\"pokok\",\"shoope\",\"shoppe\",\"shope\"\n",
    "                 ]\n",
    "        \n",
    "        with open('stopwords/20190327_stopword_id.txt') as sw:  # use the stopword list\n",
    "            new_sw = sw.readlines()\n",
    "            new_sw = [value.replace('\\n', '').strip() for value in new_sw]\n",
    "            \n",
    "            new_sw = new_sw + add_sw              # add the additional stopwords\n",
    "            \n",
    "        new_sw.extend(super().get_stop_words())   # combine stopword from Sastrawi and custom stopwords\n",
    "        return list(set(new_sw))                  # using set() to remove duplication values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff419a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sastrawi stopwords object\n",
    "sw_remover = ExtendStopWordRemoverFactory().create_stop_word_remover()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8a6e9",
   "metadata": {},
   "source": [
    "### text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa057fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def clean_text(text, sw_remover):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()                                               # case folding\n",
    "    \n",
    "    text = re.sub('#\\w+', '', text)                                        # remove all hashtags\n",
    "    text = re.sub('@\\w+', '', text)                                        # remove all tagged name\n",
    "    \n",
    "    text = re.sub('\\[.*?\\]', '', text)                                     # remove punctuation\n",
    "    text = re.sub('<.*?>+', '', text)                                      # remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)        # remove punctuation\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)                       # remove all url starts with http or https\n",
    "    text = re.sub('\\n', '', text)                                          # remove newline character \n",
    "    text = re.sub('\\w*\\d\\w*', '', text)                                    # remove any word with number character in the middle\n",
    "    text = re.sub('[^0-9a-zA-Z ]*', '', text)                              # remove all non-alphanumerical characters\n",
    "    \n",
    "    # Define the words to be replaced in a dictionary\n",
    "    words_to_replace = {\n",
    "        \"yg\": \"\",\n",
    "        \"yang\": \"\",\n",
    "        \"di\": \"\",\n",
    "        \"jadi\": \"\",\n",
    "        \"itu\": \"\",\n",
    "        \"ini\": \"\"\n",
    "    }\n",
    "    \n",
    "    for word, replacement in words_to_replace.items():\n",
    "        text = text.replace(word, replacement)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'\\s\\s+', ' ', text)                                     # remove all consecutive whitespaces into only single whitespace \n",
    "    text = stemmer.stem(text)                                              # convert words into the stem form\n",
    "    text = sw_remover.remove(text)                                         # remove stopwords\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "# this part takes some time (abt 20 mins)\n",
    "tweets['cleaned_tweet'] = tweets['tweet'].apply(lambda x: clean_text(x, sw_remover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hashtags\n",
    "tweets['hashtags'] = tweets['tweet'].apply(lambda x: re.findall(r'#\\w+', x.lower())) # Find all hashtags in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df917031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tagged name\n",
    "tweets['tagged_name'] = tweets['tweet'].apply(lambda x: re.findall(r'@\\w+', x.lower())) # Find all hashtags in the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250ca5e",
   "metadata": {},
   "source": [
    "# 4. EDA (after text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e629213",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_num_words'] = tweets['cleaned_tweet'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "tweets_negatif = tweets[tweets['sentimen']=='negatif']\n",
    "tweets_netral = tweets[tweets['sentimen']=='netral']\n",
    "tweets_positif = tweets[tweets['sentimen']=='positif']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767311fd",
   "metadata": {},
   "source": [
    "## 4.1. Distribution of tweets length\n",
    "### 4.1.1. All Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tweets['clean_num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815de9d1",
   "metadata": {},
   "source": [
    "### 4.1.2. Negatif Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tweets_negatif['clean_num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993265d3",
   "metadata": {},
   "source": [
    "### 4.1.3. Netral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tweets_netral['clean_num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3603c7",
   "metadata": {},
   "source": [
    "### 4.1.4. Positif Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d039e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tweets_positif['clean_num_words'], kde=True)\n",
    "plt.title('Distribution of tweets length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58df56",
   "metadata": {},
   "source": [
    "## 4.2. WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(data):\n",
    "    \"\"\"\n",
    "    input data from Counter object\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,10))\n",
    "#     wc = WordCloud(background_color='white', stopword = ,max_words=50).generate(data)\n",
    "    wc = WordCloud(background_color='white', max_words=200).generate_from_frequencies(data)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_top_n_words(tokenized_words, n):\n",
    "    word_counts = Counter(tokenized_words)\n",
    "    top_n_words = word_counts.most_common(n)\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3616fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460de2f",
   "metadata": {},
   "source": [
    "### 4.2.1. All  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87410020",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tweet = tweets['cleaned_tweet'].values.tolist()\n",
    "text_tweet = ' '.join(text_tweet).lower()\n",
    "tokenized_text_tweet = tokenizer.tokenize(text_tweet)\n",
    "word_counts = Counter(tokenized_text_tweet)\n",
    "plot_word_cloud(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013006d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N words\n",
    "top_n_words = count_top_n_words(tokenized_text_tweet, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= 'b');\n",
    "ax.set_title('Top words in tweet all sentiment (excluding stop words)');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('Word');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e39e38",
   "metadata": {},
   "source": [
    "### 4.2.2. Negatif Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477eec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tweet = tweets_negatif['cleaned_tweet'].values.tolist()\n",
    "text_tweet = ' '.join(text_tweet).lower()\n",
    "tokenized_text_tweet = tokenizer.tokenize(text_tweet)\n",
    "word_counts = Counter(tokenized_text_tweet)\n",
    "plot_word_cloud(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N words\n",
    "top_n_words = count_top_n_words(tokenized_text_tweet, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#FF33BB');\n",
    "ax.set_title('Top words in negative sentiment tweets (excluding stop words)');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('Word');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae5e05",
   "metadata": {},
   "source": [
    "### 4.2.3. Netral Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tweet = tweets_netral['cleaned_tweet'].values.tolist()\n",
    "text_tweet = ' '.join(text_tweet).lower()\n",
    "tokenized_text_tweet = tokenizer.tokenize(text_tweet)\n",
    "word_counts = Counter(tokenized_text_tweet)\n",
    "plot_word_cloud(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N words\n",
    "top_n_words = count_top_n_words(tokenized_text_tweet, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#AAAAAA');\n",
    "ax.set_title('Top words in netral sentiment tweets (excluding stop words)');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('Word');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a376cf7",
   "metadata": {},
   "source": [
    "### 4.2.4. Positif Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tweet = tweets_positif['cleaned_tweet'].values.tolist()\n",
    "text_tweet = ' '.join(text_tweet).lower()\n",
    "tokenized_text_tweet = tokenizer.tokenize(text_tweet)\n",
    "word_counts = Counter(tokenized_text_tweet)\n",
    "plot_word_cloud(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b594819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N words\n",
    "top_n_words = count_top_n_words(tokenized_text_tweet, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#33FFBB');\n",
    "ax.set_title('Top words in positive sentiment tweets (excluding stop words)');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('Word');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe2fcf",
   "metadata": {},
   "source": [
    "## 4.3. Hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100b3ed",
   "metadata": {},
   "source": [
    "### 4.3.1. All  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7489965",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = []\n",
    "_ = tweets['hashtags'].apply(lambda dt: all_hashtags.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_hashtags, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= 'b');\n",
    "ax.set_title('Top hashtag within all tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('hashtag');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c49bd",
   "metadata": {},
   "source": [
    "### 4.3.2. Negative  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = []\n",
    "_ = tweets_negatif['hashtags'].apply(lambda dt: all_hashtags.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_hashtags, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#FF33BB');\n",
    "ax.set_title('Top hashtag within negative sentiment tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('hashtag');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3fa09",
   "metadata": {},
   "source": [
    "### 4.3.3. Netral  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04448bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = []\n",
    "_ = tweets_netral['hashtags'].apply(lambda dt: all_hashtags.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_hashtags, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#AAAAAA');\n",
    "ax.set_title('Top hashtag within netral sentiment tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('hashtag');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438193cd",
   "metadata": {},
   "source": [
    "### 4.3.4. Positive  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fe0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = []\n",
    "_ = tweets_positif['hashtags'].apply(lambda dt: all_hashtags.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_hashtags, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#33FFBB');\n",
    "ax.set_title('Top hashtag within netral sentiment tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('hashtag');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424a335",
   "metadata": {},
   "source": [
    "# 4.4. Tagged Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56d7f1",
   "metadata": {},
   "source": [
    "### 4.4.1. All  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tagged = []\n",
    "_ = tweets['tagged_name'].apply(lambda dt: all_tagged.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_tagged, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= 'b');\n",
    "ax.set_title('Top Tagged Names within all tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('tagged name');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40addca7",
   "metadata": {},
   "source": [
    "### 4.4.2. Negative  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be7589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tagged = []\n",
    "_ = tweets_negatif['tagged_name'].apply(lambda dt: all_tagged.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_tagged, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#FF33BB');\n",
    "ax.set_title('Top Tagged Names within negative sentiment tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('tagged name');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3cab1",
   "metadata": {},
   "source": [
    "### 4.4.3. Netral  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tagged = []\n",
    "_ = tweets_netral['tagged_name'].apply(lambda dt: all_tagged.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_tagged, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#AAAAAA');\n",
    "ax.set_title('Top Tagged Names within netral sentiment tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('tagged name');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9fe75a",
   "metadata": {},
   "source": [
    "### 4.4.4. Positive  Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tagged = []\n",
    "_ = tweets_positif['tagged_name'].apply(lambda dt: all_tagged.extend(dt))\n",
    "\n",
    "\n",
    "# Top N words\n",
    "top_n_words = count_top_n_words(all_tagged, 30)  # get top 15 words\n",
    "\n",
    "df_top_words = pd.DataFrame({\n",
    "    'word':[x[0] for x in top_n_words],\n",
    "    'num':[x[1] for x in top_n_words] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(data = df_top_words, y = 'word', x = 'num', color= '#33FFBB');\n",
    "ax.set_title('Top Tagged Names within positive sentiment tweets');\n",
    "ax.set_xlabel('Number of occurences');\n",
    "ax.set_ylabel('tagged name');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f48e56",
   "metadata": {},
   "source": [
    "# 5. Advanced Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa49d8",
   "metadata": {},
   "source": [
    "## 5.0. Change sentiment to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278c9e5",
   "metadata": {},
   "source": [
    "positif    to 1\n",
    "\n",
    "netral     to 0\n",
    "\n",
    "negatif    to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82143800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the 'sentimen' column using the specified replacements\n",
    "tweets['sentimen'].replace({\"negatif\": -1, \"netral\": 0, \"positif\": 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d13f1d",
   "metadata": {},
   "source": [
    "## 5.1. Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808a604",
   "metadata": {},
   "source": [
    "50% for the training set\n",
    "\n",
    "25% for the validation set\n",
    "\n",
    "25% for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = tweets[['cleaned_tweet']]\n",
    "labels = tweets[\"sentimen\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=23)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state=23)\n",
    "\n",
    "print(\"Data distribution:\\n- Train: {} \\n- Validation: {} \\n- Test: {}\".format(len(y_train),len(y_val),len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337188b",
   "metadata": {},
   "source": [
    "## 5.2. Text Vectorization (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ae30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize(data, tfidf_vect_fit):\n",
    "    X_tfidf = tfidf_vect_fit.transform(data)\n",
    "    words = tfidf_vect_fit.get_feature_names_out()\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "    X_tfidf_df.columns = words\n",
    "    return(X_tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69023607",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['cleaned_tweet'])\n",
    "X_train = vectorize(X_train['cleaned_tweet'], tfidf_vect_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65952a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1403eea",
   "metadata": {},
   "source": [
    "# 6. Build Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create baseliner RF model\n",
    "base_rf = RandomForestClassifier(random_state = 23)\n",
    "scores = cross_val_score(base_rf, X_train, y_train.values.ravel(), cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55120e6f",
   "metadata": {},
   "source": [
    "### Learning Curve (base model RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# initiate RF model (with best params)\n",
    "# best_rf = RandomForestClassifier(n_estimators=500, max_depth=20, min_samples_split=2, max_features= 'log2')\n",
    "base_rf = RandomForestClassifier(random_state = 23)\n",
    "# Define the train_sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(base_rf, X, Y, train_sizes=train_sizes, cv=5)\n",
    "\n",
    "# Calculate the mean and standard deviation of the training and test scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training Score')\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='g', label='Validation Score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(random_state=23)\n",
    "parameters = {\n",
    "    'n_estimators': [50,100,300,500],\n",
    "    'max_depth': [2,5,10,20],\n",
    "    'min_samples_split': [2, 5, 10],  # kalo lama bgt dicomment saja\n",
    "#     'min_samples_leaf': [1, 2, 4]     # kalo lama bgt dicomment saja\n",
    "    \"max_features\": ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(rf, parameters, cv = 5)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_test_score = []\n",
    "# std_test_score = []\n",
    "# max_depth = []\n",
    "# min_samples_split = []\n",
    "# n_estimators = []\n",
    "\n",
    "\n",
    "# for idx in range(len(cv.cv_results_['std_test_score'])):\n",
    "#     max_depth.append(cv.cv_results_['params'][idx]['max_depth'])\n",
    "#     min_samples_split.append(cv.cv_results_['params'][idx]['min_samples_split'])\n",
    "#     n_estimators.append(cv.cv_results_['params'][idx]['n_estimators'])\n",
    "#     mean_test_score.append(cv.cv_results_['mean_test_score'][idx])\n",
    "#     std_test_score.append(cv.cv_results_['std_test_score'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e051145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tuning_results = pd.DataFrame(cv.cv_results_).sort_values('rank_test_score')\n",
    "df_tuning_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e6414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({\n",
    "#     'max_depth':max_depth,\n",
    "#     'min_samples_split':min_samples_split,\n",
    "#     'n_estimators':n_estimators,\n",
    "#     'mean_test_score':mean_test_score,\n",
    "#     'std_test_score':std_test_score\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2559df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement vectorization to validation set\n",
    "X_val = vectorize(X_val['cleaned_tweet'], tfidf_vect_fit)\n",
    "\n",
    "# evaluate the model using test set\n",
    "X_test = vectorize(X_test['cleaned_tweet'],tfidf_vect_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da5a50",
   "metadata": {},
   "source": [
    "### Learning Curve (after tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21feb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_val])\n",
    "Y = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71867e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# initiate RF model (with best params)\n",
    "best_rf = RandomForestClassifier(n_estimators=500, max_depth=20, min_samples_split=2, max_features= 'log2')\n",
    "\n",
    "# Define the train_sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(best_rf, X, Y, train_sizes=train_sizes, cv=5)\n",
    "\n",
    "# Calculate the mean and standard deviation of the training and test scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training Score')\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='g', label='Validation Score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94151b1d",
   "metadata": {},
   "source": [
    "# Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# best_rf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2)\n",
    "best_rf = RandomForestClassifier(n_estimators=500, max_depth=20, min_samples_split=2, max_features= 'log2')\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names = ['negatif','netral','positif']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create baseliner RF model\n",
    "# base_rf = RandomForestClassifier(random_state = 23)\n",
    "scores = cross_val_score(best_rf, X_train, y_train.values.ravel(), cv=5)\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b46b6",
   "metadata": {},
   "source": [
    "# Next Improvements\n",
    "- try change the word vectorization using GloVe or Word2Vec\n",
    "- handle slang word\n",
    "- add topic modeling for better EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d54a55",
   "metadata": {},
   "source": [
    "## LSTM Based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f208e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5aa626",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = tweets['cleaned_tweet']\n",
    "target = tweets['sentimen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28863a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
